{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from data_cube import DataCube\n",
    "from ssm import SSM\n",
    "from similarity_network_fusion import SNF, cumulated_euc_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import dtw, dtw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DataCube(\n",
    "    subjects=\"all\",\n",
    "    gestures=[\"3\", \"4\", \"5\", \"6\"],\n",
    "    channels=[\"2\", \"4\", \"6\", \"8\"],\n",
    "    data_grp=\"parsed\"\n",
    ")\n",
    "dc.load_data()\n",
    "dc.rms_smooth(300, 50)\n",
    "dc.normalize_modalities(smooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_antagonist_channels(gnum, array):\n",
    "    \"\"\"\n",
    "    pass antagonist only channels for each gesture\n",
    "    gnum - gesture number (i.e. 1, 2, 3, or 4)\n",
    "    array - entire array of all channels and time index\n",
    "    \"\"\"\n",
    "    antgnsts = {\"3\":[0, 2, 3], # channels 4 & 6; 0 is tidx\n",
    "                \"4\":[0, 1, 4], # channels 2 & 8; 0 is tidx\n",
    "                \"5\":[0, 3, 4], # channels 6 & 8; 0 is tidx\n",
    "                \"6\":[0, 1, 2]} # channels 4 & 2; 0 is tidx\n",
    "    antagonist_array = np.c_[array[:, antgnsts[gnum][0]],\n",
    "                             array[:, antgnsts[gnum][1]],\n",
    "                             array[:, antgnsts[gnum][2]]]\n",
    "    return antagonist_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dict_antag = {}\n",
    "for s, gdict in dc.data_set_smooth.items():\n",
    "    print(s)\n",
    "    dict_antag[s] = {}\n",
    "    for g, a in gdict.items():\n",
    "        snf = SNF(pass_antagonist_channels(g[0], a), k=0.5, metric=cumulated_euc_ts)\n",
    "        #snf = SNF(a[:,1:-1], k=0.5, metric=cumulated_euc_ts)\n",
    "        # calculate graph weights to find knn\n",
    "        snf.calc_weights()\n",
    "        snf.normalize_weights()\n",
    "        # generate and normalize knn graphs\n",
    "        snf.calc_knn_weights()\n",
    "        snf.normalize_knn_weights()\n",
    "        # fuse graphs\n",
    "        snf.network_fusion(eta=0.001, iters=20)\n",
    "        print(f\"subject {s}; gesture {g}\")\n",
    "        snf.plot_template()\n",
    "        # save template to dict\n",
    "        dict_antag[s][g] = snf.fused_similarity_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = dict_antag[\"30\"][\"4_0_1\"]\n",
    "plt.imshow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import lower_star_img\n",
    "from ripser import Rips\n",
    "from persim import plot_diagrams, PersImage, bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = gaussian_filter(t, sigma=3)\n",
    "plt.imshow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (t - t.mean()) / t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rips = Rips()\n",
    "dgmt = rips.fit_transform(t, distance_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn import datasets\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "# lots of random noise and 2 circles\n",
    "data = np.concatenate([150 * np.random.random((300,2)),\n",
    "                       10 + 10 * datasets.make_circles(n_samples=100)[0],\n",
    "                       100 + 20 * datasets.make_circles(n_samples=100)[0]])\n",
    "\n",
    "rips = Rips()\n",
    "dgms = rips.fit_transform(data)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(data[:,0], data[:,1], s=4)\n",
    "plt.title(\"Scatter plot of noisy data with some circles\")\n",
    "\n",
    "plt.subplot(122)\n",
    "rips.plot(dgms, legend=False, show=False)\n",
    "plt.title(\"Persistence diagram of $H_0$ and $H_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_px = 40 # persistence image dims (square)\n",
    "pim_sd = 1e-4 # persistence image st. dev.\n",
    "pim = PersImage(spread=1, pixels=[pim_px,pim_px], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagrams((dgmt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim = PersImage(spread=0.1, pixels=[50,50], verbose=False)\n",
    "img = pim.transform(dgmt[1])\n",
    "pim.show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagrams(dgms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dgms[1][:,0],dgms[1][:,1]-dgms[1][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim = PersImage(spread=1, pixels=[50,50], verbose=False)\n",
    "img = pim.transform(dgms[1])\n",
    "pim.show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### PCA on SNF Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_ant = {}\n",
    "\n",
    "for s, gdict in dict_antag.items():\n",
    "    decomp_ant[s] = {}\n",
    "    for g, a in gdict.items():\n",
    "        #print(f\"subject {s}; gesture {g}; avg {a.mean()}; sd {a.std()}\")\n",
    "        evals, evecs = la.eig(a)\n",
    "        evals = evals.real\n",
    "        sort_idx = np.argsort(-evals)\n",
    "        evals = evals[sort_idx]\n",
    "        evecs = evecs[:, sort_idx]\n",
    "        # calc percent of variance explained\n",
    "        #print(f\"subject {s}; gesture {g}; PoV: {evals[0].real / evals.real.sum()}\")\n",
    "        # do PCA\n",
    "        res = a @ evecs[:, 0]\n",
    "        res = scale(res.real)\n",
    "        decomp_ant[s][g] = res\n",
    "        # plot results\n",
    "        plt.subplot(211)\n",
    "        plt.plot(dc.data_set_smooth[s][g][:,0], res)\n",
    "        plt.subplot(212)\n",
    "        plt.plot(dc.data_set_smooth[s][g][:,0], dc.data_set_smooth[s][g][:,1])\n",
    "        plt.plot(dc.data_set_smooth[s][g][:,0], dc.data_set_smooth[s][g][:,2])\n",
    "        plt.plot(dc.data_set_smooth[s][g][:,0], dc.data_set_smooth[s][g][:,3])\n",
    "        plt.plot(dc.data_set_smooth[s][g][:,0], dc.data_set_smooth[s][g][:,4])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compare PDs between Lower Star Filtered SNF Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import persim\n",
    "from persim import plot_diagrams, PersImage, bottleneck\n",
    "from ripser import lower_star_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_lab = []\n",
    "gest_lab = []\n",
    "arrays = []\n",
    "\n",
    "for s, gdict in dict_antag.items():\n",
    "    for g, a in gdict.items():\n",
    "        subj_lab.append(s)\n",
    "        gest_lab.append(int(g[0]))\n",
    "        arrays.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bottleneck distance between all pds and make predictions\n",
    "predicts = []\n",
    "for n, g1 in enumerate(arrays):\n",
    "    print(n)\n",
    "    gaus_g1 = gaussian_filter(g1, sigma=1)\n",
    "    dgm1 = lower_star_img(gaus_g1)\n",
    "    g1_bottlenecks = []\n",
    "    for m, g2 in enumerate(arrays):\n",
    "        gaus_g2 = gaussian_filter(g2, sigma=1)\n",
    "        dgm2 = lower_star_img(gaus_g2)\n",
    "        distance_bottleneck, (matching, D) = bottleneck(dgm1, dgm2, matching=True)\n",
    "        g1_bottlenecks.append(distance_bottleneck)\n",
    "    g1_bottlenecks = np.array(g1_bottlenecks)\n",
    "    pred_idx = np.argsort(g1_bottlenecks)[1] # index of 2nd closest array by dtw; 1st closest is self\n",
    "    predicts.append(gest_lab[pred_idx])\n",
    "\n",
    "acc = (sum(np.array(gest_lab) == np.array(predicts)) / len(gest_lab)) * 100\n",
    "\n",
    "print(f\"accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compare DTW Measure between PCA SNF Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize vectors by gesture\n",
    "gest_dict = {\"3\":[], \"4\":[], \"5\":[], \"6\":[]}\n",
    "\n",
    "for s, gdict in decomp_ant.items():\n",
    "    for g, a in gdict.items():\n",
    "        if g[0] not in [\"3\", \"4\", \"5\", \"6\"]: continue\n",
    "        gest_dict[g[0]].append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " comp_dict = {\"3\":{\"3\":[], \"4\":[], \"5\":[], \"6\":[]},\n",
    "             \"4\":{\"3\":[], \"4\":[], \"5\":[], \"6\":[]},\n",
    "             \"5\":{\"3\":[], \"4\":[], \"5\":[], \"6\":[]},\n",
    "             \"6\":{\"3\":[], \"4\":[], \"5\":[], \"6\":[]}}\n",
    "\n",
    "for g1 in [\"3\", \"4\", \"5\", \"6\"]:\n",
    "    for g2 in [\"3\", \"4\", \"5\", \"6\"]:\n",
    "        for i in range(144):\n",
    "            for j in range(144):\n",
    "                if i == j and g1 == g2: continue\n",
    "                comp_dict[g1][g2].append(dtw(gest_dict[g1][i], gest_dict[g2][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g1 in [\"3\", \"4\", \"5\", \"6\"]:\n",
    "    print(f\"gesture {g1} dtw similarities w/ other gestures:\")\n",
    "    for g2 in [\"3\", \"4\", \"5\", \"6\"]:\n",
    "        print(f\"avg similarity vs gesture {g2}: {np.mean(comp_dict[g1][g2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compare Cumulated Distance between PCA SNF Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_lab = []\n",
    "gest_lab = []\n",
    "arrays = []\n",
    "\n",
    "for s, gdict in decomp_ant.items():\n",
    "    for g, a in gdict.items():\n",
    "        subj_lab.append(s)\n",
    "        gest_lab.append(int(g[0]))\n",
    "        arrays.append(a)\n",
    "\n",
    "# calculate bottleneck distance between all pds and make predictions\n",
    "predicts = []\n",
    "for n, g1 in enumerate(arrays):\n",
    "    g1_dists = []\n",
    "    for m, g2 in enumerate(arrays):\n",
    "        dist = cumulated_euc_ts(g1, g2)\n",
    "        g1_dists.append(dist)\n",
    "    g1_dists = np.array(g1_dists)\n",
    "    pred_idx = np.argsort(g1_dists)[1] # index of 2nd closest array by dtw; 1st closest is self\n",
    "    predicts.append(gest_lab[pred_idx])\n",
    "\n",
    "acc = (sum(np.array(gest_lab) == np.array(predicts)) / len(gest_lab)) * 100\n",
    "\n",
    "print(f\"accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compare PDs of Time Series Sublevel Sets after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser, Rips\n",
    "from persim import plot_diagrams, PersImage, bottleneck\n",
    "from TDA_helper_fcns import sublevel_set_time_series_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px = 20\n",
    "sd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_lab = []\n",
    "gest_lab = []\n",
    "arrays = []\n",
    "\n",
    "for s, gdict in decomp_ant.items():\n",
    "    for g, a in gdict.items():\n",
    "        subj_lab.append(s)\n",
    "        gest_lab.append(int(g[0]))\n",
    "        arrays.append(a)\n",
    "\n",
    "# calculate bottleneck distance between all pds and make predictions\n",
    "predicts = []\n",
    "for n, g1 in enumerate(arrays):\n",
    "    rips = Rips(maxdim=0, verbose=False) # initialize rips complex\n",
    "    sls1 = sublevel_set_time_series_dist(g1)\n",
    "    dgm1 = rips.fit_transform(sls1, distance_matrix=True)[0]\n",
    "    g1_bottlenecks = []\n",
    "    for m, g2 in enumerate(arrays):\n",
    "        sls2 = sublevel_set_time_series_dist(g2)\n",
    "        dgm2 = rips.fit_transform(sls2, distance_matrix=True)[0]\n",
    "        distance_bottleneck, (matching, D) = bottleneck(dgm1, dgm2, matching=True)\n",
    "        g1_bottlenecks.append(distance_bottleneck)\n",
    "    g1_bottlenecks = np.array(g1_bottlenecks)\n",
    "    pred_idx = np.argsort(g1_bottlenecks)[1] # index of 2nd closest array by dtw; 1st closest is self\n",
    "    predicts.append(gest_lab[pred_idx])\n",
    "\n",
    "acc = (sum(np.array(gest_lab) == np.array(predicts)) / len(gest_lab)) * 100\n",
    "\n",
    "print(f\"accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SW1Pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = gest_dict[\"3\"][0]\n",
    "t2 = gest_dict[\"4\"][1]\n",
    "t3 = gest_dict[\"5\"][20]\n",
    "t4 = gest_dict[\"6\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import scipy.interpolate as interp\n",
    "\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSlidingWindow(x, dim, Tau, dT):\n",
    "    N = len(x)\n",
    "    NWindows = int(np.floor((N-dim*Tau)/dT)) # The number of windows\n",
    "    if NWindows <= 0:\n",
    "        print(\"Error: Tau too large for signal extent\")\n",
    "        return np.zeros((3, dim))\n",
    "    X = np.zeros((NWindows, dim)) # Create a 2D array which will store all windows\n",
    "    idx = np.arange(N)\n",
    "    for i in range(NWindows):\n",
    "        # Figure out the indices of the samples in this window\n",
    "        idxx = dT*i + Tau*np.arange(dim) \n",
    "        start = int(np.floor(idxx[0]))\n",
    "        end = int(np.ceil(idxx[-1]))+2\n",
    "        if end >= len(x):\n",
    "            X = X[0:i, :]\n",
    "            break\n",
    "        # Do spline interpolation to fill in this window, and place\n",
    "        # it in the resulting array\n",
    "        X[i, :] = interp.spline(idx[start:end+1], x[start:end+1], idxx)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = getSlidingWindow(x=t4, dim=20, Tau=1, dT=0.5)\n",
    "PDs = ripser(sw, maxdim=1)['dgms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "Y = pca.fit_transform(sw)\n",
    "eigs = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gridspec.GridSpec(2, 2)\n",
    "plt.figure(figsize=(10,10))\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax1.scatter(Y[:, 0], Y[:, 1])\n",
    "\n",
    "ax1 = plt.subplot(gs[0,1])\n",
    "ax2 = plot_diagrams(PDs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gest_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
