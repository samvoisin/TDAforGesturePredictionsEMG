{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from data_cube import DataCube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import dtw, dtw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DataCube(\n",
    "    subjects=\"all\",\n",
    "    gestures=[\"3\", \"4\", \"5\", \"6\"],\n",
    "    channels=[\"2\", \"4\", \"6\", \"8\"],\n",
    "    data_grp=\"parsed\"\n",
    ")\n",
    "dc.load_data()\n",
    "dc.rms_smooth(100, 20)\n",
    "dc.normalize_modalities(smooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulated_euc_ts(i, j):\n",
    "    \"\"\"\n",
    "    cumulated version of the time series w/ euclidean distance\n",
    "    in which we take the sum values over time as time increases\n",
    "    and then apply the chosen metric.\n",
    "    i, j - arrays of data points\n",
    "    \"\"\"\n",
    "    # abs equivalent to ((i-j)**2)**0.5 in scalar case\n",
    "    return abs(i.sum() - j.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw channel 1 accuracy: 40.625%\n",
      "raw channel 2 accuracy: 40.45138888888889%\n",
      "raw channel 3 accuracy: 37.789351851851855%\n",
      "raw channel 4 accuracy: 34.72222222222222%\n"
     ]
    }
   ],
   "source": [
    "subj_lab = []\n",
    "gest_lab = []\n",
    "arrays = []\n",
    "\n",
    "for chan in [1,2,3,4]:\n",
    "    # get arrays of only one channel\n",
    "    for s, gdict in dc.data_set_smooth.items():\n",
    "        for g, a in gdict.items():\n",
    "            subj_lab.append(s)\n",
    "            gest_lab.append(int(g[0]))\n",
    "            arrays.append(a[:, chan])\n",
    "\n",
    "    # calculate dtw between all arrays and make predictions\n",
    "    predicts = []\n",
    "    for n, g1 in enumerate(arrays):\n",
    "        g1_dtws = []\n",
    "        for m, g2 in enumerate(arrays):\n",
    "            g1_dtws.append(cumulated_euc_ts(g1, g2))\n",
    "        g1_dtws = np.array(g1_dtws)\n",
    "        pred_idx = np.argsort(g1_dtws)[1] # index of 2nd closest array by dtw; 1st closest is self\n",
    "        predicts.append(gest_lab[pred_idx])\n",
    "\n",
    "    acc = (sum(np.array(gest_lab) == np.array(predicts)) / len(gest_lab)) * 100\n",
    "    \n",
    "    print(f\"raw channel {chan} accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### CV to determine optimal smoothing for raw signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_raw(chan, sze, stp):\n",
    "    \n",
    "    dc.rms_smooth(sze, stp)\n",
    "    dc.normalize_modalities(smooth=True)\n",
    "    \n",
    "    for s, gdict in dc.data_set_smooth.items():\n",
    "            for g, a in gdict.items():\n",
    "                subj_lab.append(s)\n",
    "                gest_lab.append(int(g[0]))\n",
    "                arrays.append(a[:, chan])\n",
    "\n",
    "    # calculate dtw between all arrays and make predictions\n",
    "    predicts = []\n",
    "    for n, g1 in enumerate(arrays):\n",
    "        g1_dtws = []\n",
    "        for m, g2 in enumerate(arrays):\n",
    "            g1_dtws.append(cumulated_euc_ts(g1, g2))\n",
    "        g1_dtws = np.array(g1_dtws)\n",
    "        pred_idx = np.argsort(g1_dtws)[1] # index of 2nd closest array by dtw; 1st closest is self\n",
    "        predicts.append(gest_lab[pred_idx])\n",
    "\n",
    "    acc = (sum(np.array(gest_lab) == np.array(predicts)) / len(gest_lab)) * 100\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "size_vals = [5*x for x in range(5, 65, 5)]\n",
    "step_vals = [x for x in range(5, 26)]\n",
    "\n",
    "tnsr = np.zeros(shape=(1, len(size_vals), len(step_vals)))\n",
    "\n",
    "for c, chan in enumerate([1]):\n",
    "    print(c)\n",
    "    for z, sze in enumerate(size_vals):\n",
    "        for t, stp in enumerate(step_vals):\n",
    "            tnsr[c,z,t] = cross_val_raw(chan, sze, stp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Matching with bottleneck distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser, Rips\n",
    "from persim import plot_diagrams, PersImage, bottleneck\n",
    "from TDA_helper_fcns import sublevel_set_time_series_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px = 20\n",
    "sd = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_lab = []\n",
    "gest_lab = []\n",
    "arrays = []\n",
    "\n",
    "for chan in [1,2,3,4]:\n",
    "    # get arrays of only one channel\n",
    "    for s, gdict in dc.data_set_smooth.items():\n",
    "        for g, a in gdict.items():\n",
    "            subj_lab.append(s)\n",
    "            gest_lab.append(int(g[0]))\n",
    "            arrays.append(a[:, chan])\n",
    "\n",
    "    # calculate bottleneck distance between all pds and make predictions\n",
    "    predicts = []\n",
    "    for n, g1 in enumerate(arrays):\n",
    "        rips = Rips(maxdim=0, verbose=False) # initialize rips complex\n",
    "        sls1 = sublevel_set_time_series_dist(g1)\n",
    "        dgm1 = rips.fit_transform(sls1, distance_matrix=True)[0]\n",
    "        g1_bottlenecks = []\n",
    "        for m, g2 in enumerate(arrays):\n",
    "            sls2 = sublevel_set_time_series_dist(g2)\n",
    "            dgm2 = rips.fit_transform(sls2, distance_matrix=True)[0]\n",
    "            distance_bottleneck, (matching, D) = bottleneck(dgm1, dgm2, matching=True)\n",
    "            g1_bottlenecks.append(distance_bottleneck)\n",
    "        g1_bottlenecks = np.array(g1_bottlenecks)\n",
    "        pred_idx = np.argsort(g1_bottlenecks)[1] # index of 2nd closest array by dtw; 1st closest is self\n",
    "        predicts.append(gest_lab[pred_idx])\n",
    "\n",
    "    acc = (sum(np.array(gest_lab) == np.array(predicts)) / len(gest_lab)) * 100\n",
    "    \n",
    "    print(f\"raw channel {chan} accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
